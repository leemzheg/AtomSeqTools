#! /usr/bin/env python3
# -*- coding: UTF-8 -*-
# @Author : chenzemin
# @E-mail : zemin-chen@ebiotron.com

import os
import sys
import argparse, textwrap
import re
import yaml
import glob
import gzip
import gffutils
import logging

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s : %(levelname)s : %(message)s",
    datefmt="%Y/%m/%d %H:%M:%S",
)
logger = logging.getLogger(__name__)


def argparse_line():
    parser = argparse.ArgumentParser(
        description="Detect fusion for ATOM-seq RNAseq",
        formatter_class=argparse.RawTextHelpFormatter,
    )
    parser.add_argument(
        "--fastq-prefix",
        metavar="STRING",
        nargs="+",
        help="specify fastq to be analyzed in {fastq-dir}\n" "[default:all]",
    )
    parser.add_argument(
        "--batch-prefix",
        metavar="STRING",
        help="specify batch analyzed prefix in sample output dir\n"
        "[default:datetime]",
    )
    parser.add_argument(
        "--data-size",
        metavar="G",
        help="specify how many data size(G) to be processed "
        "[default:0 means process all data]",
        type=float,
        default=0,
    )
    parser.add_argument(
        "--supporting-reads",
        metavar="SUPPORTING_READS",
        help="only output consensus reads/pairs that "
        "merged by >= <supporting_reads> reads/pairs [default:1]",
        type=int,
        default=1,
    )
    parser.add_argument(
        "--min-junction-reads",
        metavar="MIN_JUNCTION_READS",
        help="minimum number of junction reads required [default:5]",
        type=int,
        default=5,
    )
    parser.add_argument(
        "--min-fusion-frequency",
        metavar="MIN_FREQUENCY",
        help="minimum fusion frequency [default:0.01]",
        type=float,
        default=0.01,
    )
    parser.add_argument(
        "--min-primer-depth",
        metavar="MIN_PRIMER_DEPTH",
        help="minimum fusion all primer depth [default:50]",
        type=int,
        default=50,
    )
    parser.add_argument(
        "--auto-remove",
        action="store_true",
        help="remove all files generated by the workflow except protected",
    )
    ###############################require#################################
    required = parser.add_argument_group("required arguments")
    required.add_argument(
        "--image", metavar="IMAGE", help="singularity image [*.sif]", required=True
    )
    required.add_argument(
        "--fastq-dir",
        metavar="PATH",
        nargs="+",
        help="directory containing fastq file",
        required=True,
    )
    required.add_argument(
        "--call-variants-lib-dir",
        metavar="PATH",
        help="directory containing genome lib\n"
        "Easiest - get plug-n-play version from:\n"
        "< https://data.broadinstitute.org/Trinity/CTAT_RESOURCE_LIB/ >",
        required=True,
    )
    required.add_argument(
        "--primer-bed", metavar="BED", help="primer bed file", required=True
    )
    required.add_argument(
        "--output-dir", metavar="PATH", help="output directory", required=True
    )
    argv = vars(parser.parse_args())
    return argv


def check_gtf_sqldb(call_variants_lib_dir):
    Chromosome = [
        "chr1",
        "chr2",
        "chr3",
        "chr4",
        "chr5",
        "chr6",
        "chr7",
        "chr8",
        "chr9",
        "chr10",
        "chr11",
        "chr12",
        "chr13",
        "chr14",
        "chr15",
        "chr16",
        "chr17",
        "chr18",
        "chr19",
        "chr20",
        "chr21",
        "chr22",
        "chrX",
        "chrY",
        "chrM",
    ]

    assembly_report_file = "/opt/conda/bin/GRCh38_latest_assembly_report.txt"
    gtf_file = "/opt/conda/bin/GRCh38_latest_genomic.gtf.gz"

    gtf_sqldb = os.path.join(call_variants_lib_dir, "ref_annot.gtf.sqldb")
    if not os.path.isfile(gtf_sqldb):
        logger.warning("\033[33mGTF sqlite3 database doesn't exist\033[0m")
        logger.info(
            f"Start creating GTF sqlite3 database "
            f"to {call_variants_lib_dir} automatically (about 20min)"
        )
        ################################################################
        if os.access(call_variants_lib_dir, os.W_OK):
            relationship = {}
            with open(assembly_report_file, "r") as handle:
                for line in handle:
                    newline = line.strip().split("\t")
                    if re.match("^#", line):
                        continue
                    UCSC_style_name = newline[9]
                    RefSeq_Accn = newline[6]
                    if UCSC_style_name in Chromosome:
                        relationship[RefSeq_Accn] = UCSC_style_name
            ################################################################
            target_gtf = ""
            with gzip.open(gtf_file, "rt") as handle:
                for line in handle:
                    newline = line.strip().split("\t")
                    if re.match("^#", line):
                        target_gtf += line
                    elif newline[0] in relationship:
                        newline[0] = relationship[newline[0]]
                        target_gtf += "\t".join(newline) + "\n"
            ################################################################
            db = gffutils.create_db(
                target_gtf,
                gtf_sqldb,
                from_string=True,
                checklines=1,
                keep_order=True,
                disable_infer_genes=True,
                disable_infer_transcripts=True,
            )
            logger.info("Initial database created succeeded")

            logger.info("Start creating introns")
            introns = list(db.create_introns(numeric_sort=True))
            logger.info("Introns created succeeded")

            logger.info("Start update datebase")
            db.update(
                introns,
                checklines=1,
                make_backup=False,
                keep_order=True,
                disable_infer_genes=True,
                disable_infer_transcripts=True,
            )
            logger.info("Database update succeeded")
            logger.info("GTF sqlite3 database created succeeded")
        else:
            logger.error(
                f"\033[31mPermission denied: " f"{call_variants_lib_dir}\033[0m"
            )
            sys.exit(1)
    else:
        logger.info("GTF sqlite3 database check ok")


def check_cancer_splicing_lib(call_variants_lib_dir):
    splicing_db = os.path.join(
        call_variants_lib_dir, "cancer_splicing_lib/cancer_splicing.idx"
    )
    if not os.path.isfile(splicing_db):
        logger.warning("\033[33mSplicing database doesn't exist\033[0m")
        logger.info(
            f"Start creating splicing database "
            f"to {call_variants_lib_dir} automatically"
        )
        if os.access(call_variants_lib_dir, os.W_OK):
            os.system(
                f"ctat-splicing-lib-integration.py "
                f"--cancer_introns_tsv "
                f"/opt/conda/bin/cancer_introns.*.tsv.gz "
                f"--call_variants_lib_dir "
                f"{call_variants_lib_dir} >/dev/null 2>&1"
            )
            logger.info("Splicing database created succeeded")
        else:
            logger.error(
                f"\033[31mPermission denied: " f"{call_variants_lib_dir}\033[0m"
            )
            sys.exit(1)
    else:
        logger.info("Splicing database check ok")


def get_fastq(fastq_dir, fastq_prefix, batch_prefix, output_dir):
    sample_info = {}
    for path in fastq_dir:
        for fastq_path in glob.glob(f"{path}/*_*1.f*q.gz"):
            sample_name = fastq_path.split("/")[-1].split("_")[0]
            fastq_suffix = "_" + "_".join(fastq_path.split("/")[-1].split("_")[1:])
            if sample_name not in sample_info:
                sample_info[sample_name] = {}
                sample_info[sample_name]["R1_suffix"] = {}
                sample_info[sample_name]["R1_suffix"][path] = []
                sample_info[sample_name]["R2_suffix"] = {}
                sample_info[sample_name]["R2_suffix"][path] = []
                sample_info[sample_name]["R1_suffix"][path].append(fastq_suffix)
            else:
                if path not in sample_info[sample_name]["R1_suffix"]:
                    sample_info[sample_name]["combine_fastq"] = "TRUE"
                    sample_info[sample_name]["R1_suffix"][path] = []
                    sample_info[sample_name]["R1_suffix"][path].append(fastq_suffix)
                else:
                    sample_info[sample_name]["combine_fastq"] = "TRUE"
                    sample_info[sample_name]["R1_suffix"][path].append(fastq_suffix)

        for fastq_path in glob.glob(f"{path}/*_*2.f*q.gz"):
            sample_name = fastq_path.split("/")[-1].split("_")[0]
            fastq_suffix = "_" + "_".join(fastq_path.split("/")[-1].split("_")[1:])
            if path not in sample_info[sample_name]["R2_suffix"]:
                sample_info[sample_name]["R2_suffix"][path] = []
                sample_info[sample_name]["R2_suffix"][path].append(fastq_suffix)
            else:
                sample_info[sample_name]["R2_suffix"][path].append(fastq_suffix)

    for sample in list(sample_info):
        symbol = sample.split("-")
        if fastq_prefix:
            if sample not in fastq_prefix:
                if re.search(r"B\d+", symbol[-1]):
                    combine_sample = "-".join(symbol[:-1])
                    if combine_sample not in sample_info:
                        sample_info[combine_sample] = {}
                        sample_info[combine_sample]["combine_fastq"] = "TRUE"
                        sample_info[combine_sample]["R1_suffix"] = {}
                        sample_info[combine_sample]["R2_suffix"] = {}
                        for path in sample_info[sample]["R1_suffix"]:
                            sample_info[combine_sample]["R1_suffix"][path] = []
                            sample_info[combine_sample]["R2_suffix"][path] = []
                            for suffix in sample_info[sample]["R1_suffix"][path]:
                                sample_info[combine_sample]["R1_suffix"][path].append(
                                    "-" + symbol[-1] + suffix
                                )
                            for sufffix in sample_info[sample]["R2_suffix"][path]:
                                sample_info[combine_sample]["R2_suffix"][path].append(
                                    "-" + symbol[-1] + sufffix
                                )
                        del sample_info[sample]
                    else:
                        for path in sample_info[sample]["R1_suffix"]:
                            for sufffix in sample_info[sample]["R1_suffix"][path]:
                                sample_info[combine_sample]["R1_suffix"][path].append(
                                    "-" + symbol[-1] + sufffix
                                )
                            for sufffix in sample_info[sample]["R2_suffix"][path]:
                                sample_info[combine_sample]["R2_suffix"][path].append(
                                    "-" + symbol[-1] + sufffix
                                )
                        del sample_info[sample]
        else:
            if re.search(r"B\d+", symbol[-1]):
                combine_sample = "-".join(symbol[:-1])
                if combine_sample not in sample_info:
                    sample_info[combine_sample] = {}
                    sample_info[combine_sample]["combine_fastq"] = "TRUE"
                    sample_info[combine_sample]["R1_suffix"] = {}
                    sample_info[combine_sample]["R2_suffix"] = {}
                    for path in sample_info[sample]["R1_suffix"]:
                        sample_info[combine_sample]["R1_suffix"][path] = []
                        sample_info[combine_sample]["R2_suffix"][path] = []
                        for suffix in sample_info[sample]["R1_suffix"][path]:
                            sample_info[combine_sample]["R1_suffix"][path].append(
                                "-" + symbol[-1] + suffix
                            )
                        for sufffix in sample_info[sample]["R2_suffix"][path]:
                            sample_info[combine_sample]["R2_suffix"][path].append(
                                "-" + symbol[-1] + sufffix
                            )
                    del sample_info[sample]
                else:
                    for path in sample_info[sample]["R1_suffix"]:
                        for sufffix in sample_info[sample]["R1_suffix"][path]:
                            sample_info[combine_sample]["R1_suffix"][path].append(
                                "-" + symbol[-1] + sufffix
                            )
                        for sufffix in sample_info[sample]["R2_suffix"][path]:
                            sample_info[combine_sample]["R2_suffix"][path].append(
                                "-" + symbol[-1] + sufffix
                            )
                    del sample_info[sample]

    if fastq_prefix:
        for name in fastq_prefix:
            if name not in list(sample_info):
                logger.warning(
                    f"\033[33mThis fastq-prefix: " f"{name} not found!\033[0m"
                )

    for sample in list(sample_info):
        if fastq_prefix:
            if sample not in fastq_prefix:
                del sample_info[sample]
                continue
        if "combine_fastq" in list(sample_info[sample]):
            del sample_info[sample]["combine_fastq"]
            ################################################################
            try:
                os.makedirs(f"{output_dir}/combined_fastq")
            except OSError:
                pass
            ################################################################
            logger.warning(
                f"\033[33mFound multiple fastqs "
                f"of this fastq-prefix: {sample}\033[0m"
            )
            logger.warning(
                f"\033[33mAll {sample} fastqs will be " f"combine automatically\033[0m"
            )
            r1_source_fastq = []
            r2_source_fastq = []
            for path in list(sample_info[sample]["R1_suffix"]):
                for suffix in sample_info[sample]["R1_suffix"][path]:
                    r1_source_fastq.append(path + "/" + sample + suffix)
            for path in list(sample_info[sample]["R2_suffix"]):
                for suffix in sample_info[sample]["R2_suffix"][path]:
                    r2_source_fastq.append(path + "/" + sample + suffix)
            r1_source_fastq = " ".join(r1_source_fastq)
            r2_source_fastq = " ".join(r2_source_fastq)
            r1_combined_fastq = f"{output_dir}/combined_fastq/{sample}_combined_1.fq.gz"
            r2_combined_fastq = f"{output_dir}/combined_fastq/{sample}_combined_2.fq.gz"
            if not os.path.isfile(r1_combined_fastq):
                logger.warning(
                    f"CMD : cat {r1_source_fastq} "
                    f"> {output_dir}/combined_fastq/"
                    f"{sample}_combined_1.fq.gz"
                )
                os.system(
                    f"cat {r1_source_fastq} "
                    f"> {output_dir}/combined_fastq/"
                    f"{sample}_combined_1.fq.gz"
                )
            if not os.path.isfile(r2_combined_fastq):
                logger.warning(
                    f"CMD : cat {r2_source_fastq} "
                    f"> {output_dir}/combined_fastq/"
                    f"{sample}_combined_2.fq.gz"
                )
                os.system(
                    f"cat {r2_source_fastq} "
                    f"> {output_dir}/combined_fastq/"
                    f"{sample}_combined_2.fq.gz"
                )
            sample_info[sample]["R1_suffix"] = "_combined_1.fq.gz"
            sample_info[sample]["R2_suffix"] = "_combined_2.fq.gz"
            sample_info[sample]["directory"] = output_dir + "/combined_fastq"
        else:
            for path in list(sample_info[sample]["R1_suffix"]):
                for suffix in sample_info[sample]["R1_suffix"][path]:
                    sample_info[sample]["R1_suffix"] = suffix
                sample_info[sample]["directory"] = path
            for path in list(sample_info[sample]["R2_suffix"]):
                for suffix in sample_info[sample]["R2_suffix"][path]:
                    sample_info[sample]["R2_suffix"] = suffix

    if not sample_info:
        logger.error(
            f"\033[31mThere is no FASTQ file that meets the format "
            f"requirement in this directory: {fastq_dir}, "
            f"Please check!\033[0m"
        )
        sys.exit(1)

    final_sample_info = {}
    for sample in list(sample_info.keys()):
        sample_info[sample]["prefix"] = sample
        final_sample_info[batch_prefix + sample] = sample_info[sample]

    return final_sample_info


def set_parameter(
    fastq_prefix,
    batch_prefix,
    data_size,
    supporting_reads,
    min_junction_reads,
    min_fusion_frequency,
    min_primer_depth,
    auto_remove,
    image,
    fastq_dir,
    call_variants_lib_dir,
    primer_bed,
    output_dir,
):

    if batch_prefix:
        batch_prefix = batch_prefix + "_"
    else:
        batch_prefix = ""

    if data_size:
        read_num = int(data_size * 1000000000 / (2 * 150) + 1)
    else:
        read_num = int(data_size)

    check_cancer_splicing_lib(call_variants_lib_dir)
    check_gtf_sqldb(call_variants_lib_dir)
    sample_info = get_fastq(fastq_dir, fastq_prefix, batch_prefix, output_dir)

    default = {
        "image_version": image,
        "samples": sample_info,
        "python3": "python3",
        "bedtools": "bedtools",
        "scripts": "/snakemake/bin/scripts",
        "primer_bed": primer_bed,
        "fastp": {
            "path": "fastp",
            "adapter_sequence": "AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC",
            "adapter_sequence_r2": "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT",
            "umi_loc": "read2",
            "umi_len": 9,
            "umi_prefix": "UMI",
            "qualified_quality": 15,
            "unqualified_percent": 40,
            "n_base_limit": 5,
            "length_required": 50,
            "read_num": read_num,
            "threads": 8,
        },
        "extend_umi": {"extend_length": 11},
        "star": {
            "path": "STAR",
            "genomeDir": f"{call_variants_lib_dir}/ref_genome.fa.star.idx",
            "outSAMattrRGline": "ID:flowcell.lane LB:library " "SM:sample PL:ILLUMINA",
            "twopassMode": "None",
            "genomeLoad": "LoadAndKeep",
            "limitBAMsortRAM": 10000000000,
            "threads": 8,
        },
        "gencore": {
            "path": "gencore",
            "genome": f"{call_variants_lib_dir}/ref_genome.fa",
            "supporting_reads": supporting_reads,
            "umi_diff_threshold": 1,
        },
        "intersect": {"primer_cov": 0.9, "softclip_length": 2, "distance": 2},
        "star_fusion": {
            "path": "STAR-Fusion",
            "star_fusion_lib": f"{call_variants_lib_dir}",
            "min_sum_frags": 1,
            "threads": 8,
        },
        "star_fusion_filter": {
            "min_junction_reads": f"{min_junction_reads}",
            "min_fusion_frequency": f"{min_fusion_frequency}",
            "min_primer_depth": f"{min_primer_depth}",
        },
        "ctat_splicing": {"path": "STAR_to_cancer_introns.py"},
    }

    with open(f"{output_dir}/{batch_prefix}RNA_config.yaml", "w") as output:
        yaml.dump(
            default,
            output,
            default_flow_style=False,
            allow_unicode=True,
            sort_keys=False,
        )

    logger.info("Configuration finished")
    logger.info("Start detecting gene fusion")

    os.system(
        f"snakemake -s /snakemake/bin/snakefile_fusion "
        f"--configfile {output_dir}/{batch_prefix}RNA_config.yaml "
        f"--cores all "
        f"-d {output_dir} -q progress --use-conda"
    )

    if auto_remove:
        logger.info("Start clean files")
        os.system(
            f"snakemake -s /snakemake/bin/snakefile_fusion "
            f"--configfile {output_dir}/{batch_prefix}RNA_config.yaml "
            f"--cores all "
            f"-d {output_dir} --delete-all-output"
        )
        logger.info("Clean files finished")

    for sample in sample_info:
        with open(
            f"{output_dir}/{sample}/result/" f"{sample}.RNA_config.yaml", "w"
        ) as output:
            yaml.dump(
                default,
                output,
                default_flow_style=False,
                allow_unicode=True,
                sort_keys=False,
            )

    os.system(
        f"rm -rf {output_dir}/.genomeLoad "
        f"{output_dir}/.snakemake "
        f"{output_dir}/{batch_prefix}RNA_config.yaml"
    )

    logger.info("Detect gene fusion finished")


def main():
    argv = argparse_line()
    set_parameter(
        argv["fastq_prefix"],
        argv["batch_prefix"],
        argv["data_size"],
        argv["supporting_reads"],
        argv["min_junction_reads"],
        argv["min_fusion_frequency"],
        argv["min_primer_depth"],
        argv["auto_remove"],
        argv["image"],
        argv["fastq_dir"],
        argv["call_variants_lib_dir"],
        argv["primer_bed"],
        argv["output_dir"],
    )


if __name__ == "__main__":
    main()
